{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "articles.groupby(\"date\").size().rolling(30).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_colon_splits = lambda x: x.split(\": \", 1)[0] if \": \" in x else None\n",
    "articles[\"title\"].apply(find_colon_splits).value_counts().index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f29b1d",
   "metadata": {},
   "source": [
    "I'm clustering the articles on the titles, as titles should contain the story more succinctly than the full text. \n",
    "\n",
    "Many article titles, however are in the format: `$SECTION: $THE_ACTUAL_TITLE`, indicated by the presence of a colon.\n",
    "\n",
    "This biases the embedding, as the vectorisation process takes into account pharses like \"The Guardian View\" or \"Monday Briefing\". \n",
    "\n",
    "I find all instances of colon-split titles, and make some arbitrary decisions about which to trim. My general heuristic is I remove:\n",
    "\n",
    "- Anything which is clearly a recurring feature for a paper (e.g., Wednesday briefing)\n",
    "- Anything containing `$SOMETHING_ABOUT_THE_WAR latest|live|briefing`\n",
    "- Journalistic tropes, e.g., \"Revealed\", \"Analysis\"\n",
    "\n",
    "I leave in everything else, e.g. \"Trump: I hate that Epstein guy\".\n",
    "\n",
    "The below cell identifies the parts to crop, tests to see if cropping leaves empty titles, and applies the cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_parts = [\n",
    "    \"First Thing\",\n",
    "    \"Middle East crisis live\",\n",
    "    \"Morning Mail\",\n",
    "    \"Afternoon Update\",\n",
    "    \"Watch\",\n",
    "    \"Israel-Gaza war live\",\n",
    "    \"Israel-Hamas war latest\",\n",
    "    \"Letters\",\n",
    "    \"Wednesday briefing\",\n",
    "    \"Israel-Hamas war live\",\n",
    "    \"Friday briefing\",\n",
    "    \"Tuesday briefing\",\n",
    "    \"Monday briefing\",\n",
    "    \"Australia news live\",\n",
    "    \"Thursday briefing\",\n",
    "    \"Revealed\",\n",
    "    \"Five Great Reads\",\n",
    "    \"Wednesday evening news briefing\",\n",
    "    \"Israel-Gaza latest news\",\n",
    "    \"Battle Lines\",\n",
    "    \"Friday evening news briefing\",\n",
    "    \"Monday evening news briefing\",\n",
    "    \"Thursday evening news briefing\",\n",
    "    \"Tuesday evening news briefing\",\n",
    "    \"Israel-Hamas war latest news\",\n",
    "    \"TV tonight\",\n",
    "    \"Politics latest news\",\n",
    "    \"Israel briefing\",\n",
    "    \"Israel-Palestine latest news\",\n",
    "    \"Dining across the divide\",\n",
    "    \"Pictured\",\n",
    "    \"Digested week\",\n",
    "    \"The Observer view\",\n",
    "    \"The Daily T\",\n",
    "    \"Australia politics live\",\n",
    "    \"Guardian Essential poll\",\n",
    "    \"Israel-Gaza war latest\",\n",
    "    \"Middle East live\",\n",
    "    \"The week in TV\",\n",
    "    \"The Crunch\",\n",
    "    \"On my radar\",\n",
    "    \"Russia-Ukraine war live\",\n",
    "    \"Israel-Iran war latest\",\n",
    "    \"Analysis\",\n",
    "    # \"The week in parliament\", These didn't split\n",
    "    # \"The Guardian view on war in the Middle East\", These didn't split\n",
    "]\n",
    "\n",
    "# Remove \"The Guardian view on ...\" from titles\n",
    "articles[\"title\"] = articles[\"title\"].str.replace(\n",
    "    r\"^The Guardian view on \", \"\", regex=True\n",
    ")\n",
    "\n",
    "# Get a list of all titles that start with any of the bad parts, this throws an error if it doesn't split\n",
    "bad_titles = []\n",
    "for part in bad_parts:\n",
    "    for title in articles[articles[\"title\"].str.startswith(part + \":\")][\n",
    "        \"title\"\n",
    "    ].to_list():\n",
    "        bad_titles.append((part, title.split(\":\", 1)[1]))\n",
    "\n",
    "\n",
    "def crop_part(part, title):\n",
    "    if title.startswith(part + \":\"):\n",
    "        return title.split(\":\", 1)[1].strip()\n",
    "    return title\n",
    "\n",
    "\n",
    "for part in bad_parts:\n",
    "    articles[\"title\"] = articles[\"title\"].apply(lambda x: crop_part(part, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Guardian also has a habit of including the journo in the title,\n",
    "# e.g., \"I had beans on toast for breakfast â€“ and it was a mistake | Adrian Chiles\"\n",
    "# We cut this too\n",
    "\n",
    "find_pipe_splits = lambda x: x.split(\" | \", 1)[0] if \" | \" in x else x\n",
    "articles[\"title\"] = articles[\"title\"].apply(find_pipe_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Tweaking the UMAP model to use cosine distance, which is better for text data\n",
    "# And to use 20 neighbors to capture more global structure and set random_state for reproducibility\n",
    "umap_model = UMAP(n_neighbors=15, n_components=6, random_state=42)\n",
    "\n",
    "# Picking a higher-quality model than the default \"all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Tweaking the hdbscan model to provide slightly larger, denser clusters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    "    min_samples=5,\n",
    ")\n",
    "\n",
    "# Using a CountVectorizer to remove English stop words in topic representation\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "\n",
    "# Clustering on title, which should capture the main event for each article\n",
    "docs = articles[\"title\"].tolist()\n",
    "\n",
    "\n",
    "# Creating the BERTopic model with the custom components\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# HDBSCAN can produce a lot of outlier topics, I reduce these by merging them into the closest topic\n",
    "new_topics = topic_model.reduce_outliers(\n",
    "    docs,\n",
    "    topics,\n",
    "    probabilities=probs,\n",
    "    strategy=\"probabilities\",\n",
    ")\n",
    "\n",
    "# Update the model with the outlier-reduced topics\n",
    "topic_model.update_topics(docs, topics=new_topics)\n",
    "\n",
    "# Building a hierarchy of topics\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs, new_topics)\n",
    "\n",
    "# Assigning topics back to the dataframe\n",
    "articles[\"topic_n\"] = new_topics\n",
    "\n",
    "# Add in topic labels\n",
    "articles[\"topic_label\"] = articles[\"topic_n\"].map(\n",
    "    topic_model.get_topic_info().set_index(\"Topic\")[\"Name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchical_documents(\n",
    "    articles[\"title\"].to_list(), hierarchical_topics, hide_document_hover=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae478ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61021541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9a275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fdcea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
